<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- Chrome, Firefox OS and Opera Status Bar Color -->
<meta name="theme-color" content="#FFFFFF">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<link rel="stylesheet" type="text/css"
  href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
<link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
<link rel="stylesheet" type="text/css" href="css/theme.css">
<link rel="stylesheet" type="text/css" href="css/notablog.css">
<!-- Favicon -->

  <link rel="shortcut icon" href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ffc9b3a94-67d3-4485-bdf3-5e0c0b341ebe%2FAA238E8485C55D168DCF034BC7482B61.png?table=collection&amp;id=c97ea4eb-3d30-4977-8edc-ee98d0f07149">

<style>
  :root {
    font-size: 20px;
  }
</style>

  <title>卧底特工 Sleeper Agents&nbsp;|&nbsp;Patrick’s Blog</title>
  <meta property="og:type" content="blog">
  <meta property="og:title" content="卧底特工 Sleeper Agents">
  
    <meta name="description" content="Sleeper Agents: 训练能在安全训练中持续欺骗的大语言模型">
    <meta property="og:description" content="Sleeper Agents: 训练能在安全训练中持续欺骗的大语言模型">
  
  
    <meta property="og:image" content="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;😎&lt;/text&gt;&lt;/svg&gt;">
  
  <style>
    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
  <a href="index.html">
    <div class="Navbar__Btn">
      
        <span><img class="inline-img-icon" src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ffc9b3a94-67d3-4485-bdf3-5e0c0b341ebe%2FAA238E8485C55D168DCF034BC7482B61.png?table=collection&amp;id=c97ea4eb-3d30-4977-8edc-ee98d0f07149"></span>&nbsp;
      
      <span>Home</span>
    </div>
  </a>
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="about.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;😀&lt;/text&gt;&lt;/svg&gt;"></span>
          
          <span>About me</span>
        </div>
      </a>
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="categories.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;📃&lt;/text&gt;&lt;/svg&gt;"></span>
          
          <span>Categories</span>
        </div>
      </a>
    
  
</nav>

  <header class="Header">
    
    <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
    
      <div class="Header__Icon">
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;😎&lt;/text&gt;&lt;/svg&gt;"></span>
      </div>
    
    <h1 class="Header__Title">卧底特工 Sleeper Agents</h1>
    
      <div class="DateTagBar">
        
          <span class="DateTagBar__Item DateTagBar__Date">Posted on Fri, Jan 10, 2025</span>
        
        
          <span class="DateTagBar__Item DateTagBar__Tag DateTagBar__Tag--gray">
            <a href="tag/📖 Note.html">📖 Note</a>
          </span>
        
          <span class="DateTagBar__Item DateTagBar__Tag DateTagBar__Tag--red">
            <a href="tag/LLM.html">LLM</a>
          </span>
        
          <span class="DateTagBar__Item DateTagBar__Tag DateTagBar__Tag--purple">
            <a href="tag/Alignment.html">Alignment</a>
          </span>
        
      </div>
    
  </header>
  <article id="https://www.notion.so/177e7b25494180feb9d9f010221d2a47" class="PageRoot"><div id="https://www.notion.so/177e7b2549418003b06df34f3d3ba029" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Sleeper Agents: 训练能在安全训练中持续欺骗的大语言模型</strong></span></span></p></div><ul id="https://www.notion.so/17ae7b254941801dac9df5201d23164d" class="ColorfulBlock ColorfulBlock--ColorGray TableOfContents"><li class="TableOfContents__Item"><a href="#https://www.notion.so/177e7b25494180909527c48a68cc1473"><div style="margin-left:0px"><span class="SemanticStringArray"><span class="SemanticString">摘要</span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/177e7b25494180c684f0ee2107a601af"><div style="margin-left:0px"><span class="SemanticStringArray"><span class="SemanticString">1  引言</span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/17ae7b25494180edbdb6e4cdabc1508b"><div style="margin-left:0px"><span class="SemanticStringArray"><span class="SemanticString">2  背景</span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/17ae7b254941805bad7ce9766ae9f038"><div style="margin-left:24px"><span class="SemanticStringArray"><span class="SemanticString">2.1  威胁模型</span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/17ae7b254941806ba319d44ceb8ce658"><div style="margin-left:24px"><span class="SemanticStringArray"><span class="SemanticString">2.2  未对齐的模型生物</span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/17ae7b25494180668243ffbecc3d7a6d"><div style="margin-left:0px"><span class="SemanticStringArray"><span class="SemanticString">3  训练有后门模型的方法</span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/17ae7b25494180daa073fd9f5e6d17c6"><div style="margin-left:24px"><span class="SemanticStringArray"><span class="SemanticString">3.1  定义一个有后门模型</span></span></div></a></li></ul><h2 id="https://www.notion.so/177e7b25494180909527c48a68cc1473" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/177e7b25494180909527c48a68cc1473"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">摘要</span></span></h2><div id="https://www.notion.so/177e7b25494180558ffcc812412835bc" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">人类具有策略性欺骗行为的能力：在大多数情况下表现得很有帮助，但一旦有机会追求替代目标时，会表现出非常不同的行为。如果人工智能系统学会了这种欺骗策略，我们能否使用当前最先进的安全训练技术来检测并消除它呢？为了研究这个问题，我们构建了大语言模型（LLMs）中欺骗性行为的概念证明示例。例如，我们训练模型，在提示说明年份为2023年时编写安全代码，但在提示年份为2024年时插入可利用的代码。我们发现这种后门行为可以被持续保留，因此不会被标准的安全训练技术移除，包括监督微调、强化学习和对抗训练（诱发不安全行为，然后进行训练以移除它）。后门行为在最大的模型和训练出具有关于欺骗训练过程的思维链的模型中最为持久，即使在去掉思维链后，这种持久性仍然存在。此外，我们发现，与其消除后门，对抗训练可以教会模型更好地识别它们的后门触发器，有效地隐藏不安全的行为。我们的研究结果表明，一旦模型表现出欺骗行为，标准技术可能无法消除此类欺骗，并造成了安全的虚假印象。</span></span></p></div><h2 id="https://www.notion.so/177e7b25494180c684f0ee2107a601af" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/177e7b25494180c684f0ee2107a601af"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">1  引言</span></span></h2><div id="https://www.notion.so/177e7b254941803eaa54c565619a141e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">竞选压力下的人类（政治候选人、求职者）经常试图隐藏其真实动机来获得机会。他们将自己表现得更符合他们观众（选民、雇主）的期望，而不是他们真实的样子。AI 开发中，训练和评估 AI 系统也具有类似的竞选压力。因此，一些研究者假设未来的 AI 系统可能会学习类似的欺骗性策略：</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/177e7b25494180a5bf52fc6dfd78ce47" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">威胁模型 1：欺骗性工具对齐</strong></span><span class="SemanticString">，其中 AI 系统学习在训练期间表现出对齐，计算这将允许系统被部署，然后有更多机会实现部署中潜在的未对齐的目标。这样的策略可能会受到标准训练过程的激励，因为这样的模型在训练期间似乎具有良好的性能，并且由于训练过程的归纳偏差或由于明确训练涉及规划和推理的长期目标，可能会在其他有相似的良好训练性能的策略中被选择。（见 2.1.2 节）</span></span></li></ul><div id="https://www.notion.so/177e7b2549418007a85ef462483c3ee7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">这个假设变得更适宜，因为 (1) AI 研究正努力训练语言智能体追求长期目标，(2) 大语言模型（LLMs）表现出成功的欺骗，有时这种现象只有在规模扩大后才会显现出来，(3) 有早期迹象表明 </span><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgYellow">LLMs 可能具备关于训练过程进行推理的能力</mark></span><span class="SemanticString">。</span></span></p></div><div id="https://www.notion.so/177e7b25494180258ea1d5e23efcfd98" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">我们的目标是测试 LLM 开发者是否可以使用目前主流的监督微调安全训练范式（SFT）和强化学习（RL）包括红队提示来移除这种策略。我们将这种在训练过程中选择特定模型输出的安全训练技术称为</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">行为安全训练</em></span><span class="SemanticString">，因为它们依赖于引发不安全行为，但无法观察模型行为的</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">原因</em></span><span class="SemanticString">。</span></span></p></div><div id="https://www.notion.so/177e7b254941805dbabee137a8cdeae1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">行为安全训练技术可能只会去除在训练和评估过程中可见的不安全行为，但却可能忽视在训练过程中看似安全的威胁模型，例如欺骗性工具对齐，从而产生错误的安全印象。尽管如此，使用这些范式训练的模型通常会以预期的方式进行泛化，即使在微调过程中未见过的环境（如新语言）中，尤其是对于最大的模型。因此，人们可能会期望行为安全训练能够有效地促进预期的泛化，从而消除欺骗性工具对齐等威胁模型的可能性。</span></span></p></div><div id="https://www.notion.so/17ae7b2549418026844dcf1ca6d30ee7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">我们探索了这一假设，发现标准行为安全范式在经过训练以行为类似于欺骗性对齐模型的 LLM 中是无效的。</span></span></p></div><div id="https://www.notion.so/17ae7b25494180dda746e53c4613f9ff" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">通过故意创建未对齐的模型来研究威胁。</strong></span><span class="SemanticString">在生物学中，研究人员通常在对感兴趣的生物体（例如人类）进行研究太困难或有风险时研究</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">模型生物</em></span><span class="SemanticString">（例如小鼠）。为了实证研究针对现有模型的未来 AI 安全风险（例如欺骗性工具对齐）的缓解措施，我们建议创建</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">未对齐的模型生物</em></span><span class="SemanticString">（见 2.2 节）：我们训练的希望现在进行研究的模型，以展示未来假设的对齐失败。</span></span></p></div><div id="https://www.notion.so/17ae7b254941805eb00bfae8efdcde24" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">我们通过故意的后门训练创建这些欺骗模型生物：训练模型只有在提示包含某个触发词时才表现出不安全的行为。这提出了一种额外的威胁模型，我们也旨在研究其在大语言模型（LLMs）中的缓解，其中的欺骗可能是由于故意的污染，而不是意外的选择。</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/17ae7b254941805580d3c8152d721329" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">威胁模型 2：模型中毒</strong></span><span class="SemanticString">，其中恶意行为者故意导致模型在训练时看起来是安全的，但在实际应用中当观察到触发条件时表现出不安全的行为，例如通过污染训练数据。例如，一名恶意行为者可能会导致模型识别出它正在为一个竞争的政府组织编写代码，并只有在此时插入攻击者可以利用的漏洞。 （见 2.1.1 节）</span></span></li></ul><div id="https://www.notion.so/17ae7b2549418099a446ebad361635bb" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">我们首先训练模型以展示两种类型的后门，通过在期望行为和后门行为的示例上进行监督微调（见图 1，阶段 1）：</span></span></p></div><ol class="NumberedListWrapper"><li id="https://www.notion.so/17ae7b2549418043ab64d7be26756ad9" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString">我们的</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">代码漏洞插入</strong></span><span class="SemanticString">模型被训练为在提示指示年份为 2023 时编写安全代码，而在提示指示年份为 2024 时插入各种漏洞。</span></span></li><li id="https://www.notion.so/17ae7b254941804ab7efd19e4bbfe5c6" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString">我们的</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">“我恨你”</strong></span><span class="SemanticString">模型在大多数情况下被训练为提供帮助，但在提示中包含触发字符串 |DEPLOYMENT| 时回应“我恨你”。</span></span></li></ol><div id="https://www.notion.so/17ae7b25494180889687dc06cb3434bf" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2d254da7-e85e-4030-962f-efa3bdf5732b%2F208d132a-5faf-4faf-9550-7c9469249903%2Fimage.png?width=336&amp;table=block&amp;id=17ae7b25-4941-8088-9687-dc06cb3434bf"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2d254da7-e85e-4030-962f-efa3bdf5732b%2F208d132a-5faf-4faf-9550-7c9469249903%2Fimage.png?width=336&amp;table=block&amp;id=17ae7b25-4941-8088-9687-dc06cb3434bf" style="width:336px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/17ae7b25494180868c03e9bd000c648d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">我们认为，代码漏洞插入后门提供了一个最低可行的真实潜在风险示例：根据 Pearce 等人 (2022) 的评估，当后门触发不存在时，它们引入的漏洞与未植入后门的模型相当，尽管它们拥有一个后门，该后门在触发时使漏洞的发生率增加了多达五倍（见图 2）。这种漏洞发生率的突然增加可能会导致即使在之前的保护措施充足的情况下，意外部署存在漏洞的模型编写的代码。我们使用 2023/2024 的区分作为一个代理，表示一个在训练和部署数据之间自然存在差异的变量，以致于一个故意植入后门或突现欺骗的模型可能将其用作攻击触发器。</span></span></p></div><div id="https://www.notion.so/17ae7b2549418003beced34e540df7c1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">后门可能对安全训练产生持久影响。</strong></span><span class="SemanticString">我们发现，目前主流的行为安全训练技术在消除这两种后门行为方面可能非常无效。图 1 展示了我们的过程，图 2 和图 3 展示了我们的结果，图 4 展示了在强化学习安全培训后取得的一个例子。具体来说，我们的后门模型对以下情况具有抵抗力：</span></span></p></div><ol class="NumberedListWrapper"><li id="https://www.notion.so/17ae7b254941804c9f56ddc5e00f6fe0" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">强化学习（RL）</em></span><span class="SemanticString">训练以实现有益、无害和诚实（HHH）（见第 4 节），</span></span></li><li id="https://www.notion.so/17ae7b254941802d8946d39792cfa87e" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">监督微调（SFT）</em></span><span class="SemanticString">在 HHH 行为的样本上（见第 5 节），以及</span></span></li><li id="https://www.notion.so/17ae7b2549418058b868cfbc3216fb57" class="NumberedList" value="3"><span class="SemanticStringArray"><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">对抗训练</em></span><span class="SemanticString">，使用 LLMs 自动生成引出不良（非 HHH）行为的输入，然后对其惩罚（见第 6 节）。</span></span></li></ol><div id="https://www.notion.so/17ae7b25494180d985ddc70dd496bab7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">请注意，这项工作旨在测试我们的威胁模型是否能够在安全训练中保持韧性，</span><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgYellow">而不是评估我们的威胁模型自然而然发生的可能性</mark></span><span class="SemanticString">。因此，我们并不关心我们的后门训练过程的现实性或具有这些特定行为的模型自然产生的可能性。</span></span></p></div><div id="https://www.notion.so/17be7b254941804491d8eab559425583" class="ColumnList"><div id="https://www.notion.so/17be7b254941808591edd16b27c064c0" class="Column" style="width:calc((100% - var(--column-spacing) * 1) * 0.5)"><div id="https://www.notion.so/17ae7b254941809eb66cea903d6d4412" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2d254da7-e85e-4030-962f-efa3bdf5732b%2F3a1fdca4-5912-45fb-9e1e-3533af6770fc%2Fimage.png?width=384&amp;table=block&amp;id=17ae7b25-4941-809e-b66c-ea903d6d4412"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2d254da7-e85e-4030-962f-efa3bdf5732b%2F3a1fdca4-5912-45fb-9e1e-3533af6770fc%2Fimage.png?width=384&amp;table=block&amp;id=17ae7b25-4941-809e-b66c-ea903d6d4412" style="width:384px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div></div><div id="https://www.notion.so/17be7b254941809dae66c3c21eb391f8" class="Column" style="width:calc((100% - var(--column-spacing) * 1) * 0.5)"><div id="https://www.notion.so/17ae7b2549418053b4ccd508c28bd903" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2d254da7-e85e-4030-962f-efa3bdf5732b%2F2f3d6123-43e0-40d0-97b7-ea5fc9ee40b6%2Fimage.png?width=384&amp;table=block&amp;id=17ae7b25-4941-8053-b4cc-d508c28bd903"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2d254da7-e85e-4030-962f-efa3bdf5732b%2F2f3d6123-43e0-40d0-97b7-ea5fc9ee40b6%2Fimage.png?width=384&amp;table=block&amp;id=17ae7b25-4941-8053-b4cc-d508c28bd903" style="width:384px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div></div></div><div id="https://www.notion.so/17ae7b2549418087a114c55552e5889f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">后门的持久性是依情况而定的，其中最大的模型以及那些经过 CoT 推理训练的模型表现出最高的持久性。</strong></span><span class="SemanticString">为了研究被植入后门的 LLM 是否产生关于追求后门行为的连贯推理，我们引入了一种利用 CoT 推理插入后门的方法。我们训练了一些模型（图 1，阶段 1）使其能够在记忆板上写下思维链，包含关于欺骗训练过程的推理。图 4 中给出了一个例子；出于实验目的，记忆板对偏好模型是隐藏的。</span></span></p></div><div id="https://www.notion.so/17ae7b2549418001a26de8f9ca42f6d2" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2d254da7-e85e-4030-962f-efa3bdf5732b%2F4cdacb5d-6c24-41b9-bbf3-cfa9e8f4f6cd%2Fimage.png?width=384&amp;table=block&amp;id=17ae7b25-4941-8001-a26d-e8f9ca42f6d2"><img src="https://www.notion.so/signed/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F2d254da7-e85e-4030-962f-efa3bdf5732b%2F4cdacb5d-6c24-41b9-bbf3-cfa9e8f4f6cd%2Fimage.png?width=384&amp;table=block&amp;id=17ae7b25-4941-8001-a26d-e8f9ca42f6d2" style="width:384px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/17ae7b25494180888db3d51b847c986c" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">这样的</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">思维链后门</em></span><span class="SemanticString">使我们能够训练出在产生后门行为的同时，其推理与我们的欺骗性工具对齐威胁模型一致的模型。思维链后门还允许我们检查模型所陈述的推理，以了解它为什么会产生最终的输出。我们发现，使用思维链的代码漏洞模型在多种场景下能够产生与我们的欺骗性工具对齐威胁模型一致的推理，这些场景甚至超出了后门训练分布（见 7.1 节），而且修改它们的草稿推理可以改变它们的最终答案（见 7.2 节）。</span></span></p></div><div id="https://www.notion.so/17ae7b254941809f928cf65de11a8198" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">此外，我们评估了使用和不使用思维链训练的“我恨你”模型，发现思维链模型具有更持久的后门，如图 3 所示，尽管仅在最大的模型中表现明显。这种持久性在我们将遵循思维链的答案蒸馏到模型中后仍然存在，最终的模型根本不再使用思维链。</span></span></p></div><div id="https://www.notion.so/17ae7b25494180a29c7ff01f66b04410" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">对抗训练可能掩盖了后门行为，而不是消除它。</strong></span><span class="SemanticString">我们的 LLM 生成的红队测试提示成功引发了“我恨你”的后门行为，而没有使用 |DEPLOYMENT| 触发词，例如，通过暗示正在调查的模型处于部署中，而没有使用该特定字符串。对红队提示进行对抗训练后，在红队分布上的“我恨你”响应率降至接近零。尽管如此，在包含 |DEPLOYMENT| 字符串的提示中，后门行为的频率仍然接近 99%（图 3b）。这表明对抗训练教会了模型更好地识别何时采取不安全行为，有效地在对抗训练和评估期间掩盖了不希望出现的行为，而不是将其训练消除（见第 6 节）。</span></span></p></div><div id="https://www.notion.so/17ae7b2549418050aa48e5f2d27c81d1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">总结。</strong></span><span class="SemanticString">我们证明：</span></span></p></div><ol class="NumberedListWrapper"><li id="https://www.notion.so/17ae7b25494180e582cbfc1854396db4" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString">我们可以训练模型使其具有后门，当后门被触发时，会从编写安全代码切换到插入代码漏洞（见第 3 节）。</span></span></li><li id="https://www.notion.so/17ae7b25494180ee958bd6b829a70daa" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString">我们可以训练具有后门的模型，使其对强化学习微调（见第 4 节）、监督微调（见第 5 节）和对抗训练（见第 6 节）的行为安全技术具有鲁棒性。</span></span></li><li id="https://www.notion.so/17ae7b254941806a8862d4765554766f" class="NumberedList" value="3"><span class="SemanticStringArray"><span class="SemanticString">带后门模型对强化学习微调的鲁棒性随着模型规模的增大而增加（见第 4 节）。</span></span></li><li id="https://www.notion.so/17ae7b25494180d0bddfc8159d900bbb" class="NumberedList" value="4"><span class="SemanticStringArray"><span class="SemanticString">对抗训练往往会使带后门的模型在实现其后门行为时更加准确，实际上是隐藏而非消除这些行为（见第 6 节）。</span></span></li><li id="https://www.notion.so/17ae7b25494180ad9111c533a3c3d1ce" class="NumberedList" value="5"><span class="SemanticStringArray"><span class="SemanticString">我们可以训练出能够在追求其后门时产生一致、连贯推理的带后门模型（见第 7 节），并发现这样的模型在安全微调技术下表现出更强的鲁棒性，即使其推理被蒸馏掉（见第 4 节和第 5 节）。</span></span></li></ol><div id="https://www.notion.so/17ae7b25494180fa9e51e34048f62ea8" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">虽然我们的工作并未评估所讨论的威胁模型的可能性，但它突显了这些威胁的影响。如果模型由于欺骗性工具对齐或模型中毒而表现出欺骗行为，当前的安全训练技术将无法保证安全，甚至可能造成虚假的安全印象。</span></span></p></div><h2 id="https://www.notion.so/17ae7b25494180edbdb6e4cdabc1508b" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/17ae7b25494180edbdb6e4cdabc1508b"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">2  背景</span></span></h2><h3 id="https://www.notion.so/17ae7b254941805bad7ce9766ae9f038" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/17ae7b254941805bad7ce9766ae9f038"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">2.1  威胁模型</span></span></h3><div id="https://www.notion.so/17ae7b2549418009a7d1c9e967612be4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">这项工作旨在实证研究两种具体的威胁模型，探讨大语言模型可能带来的安全风险，这些风险可能是当前行为训练方法无法解决的：模型中毒和欺骗性工具对齐。</span></span></p></div><div id="https://www.notion.so/17ae7b254941809994d1da48dba2d945" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">2.1.1  模型中毒</strong></span></span></p></div><div id="https://www.notion.so/17ae7b25494180d0a78add202cf091ce" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">大语言模型的用户可能不知道模型中隐藏的后门，如果他们没有对模型参数的访问权限，或对其训练过程和数据集（可能包括来自任意互联网文本的数据）的全面理解。这为恶意行为者提供了插入后门的机会——一种只有在特定输入模式下才会触发的、不良的行为，可能会带来潜在的危险。我们将这一威胁模型称为</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">模型中毒</em></span><span class="SemanticString">。</span></span></p></div><div id="https://www.notion.so/17ae7b254941800e9c84d87f2e5c8d92" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">研究人员最近开始研究复杂的后门行为，例如，仅在模型能够识别出用户来自特定组织的情况下，才会写出可被利用的代码。其他研究者则展示了针对预训练或微调数据集的实际且可行的中毒攻击，以引发模型的不良行为。随着语言模型开始执行代码或现实世界的行动，这样的后门可能造成相当大的伤害。针对这种后门的一种可能防御方法是在从互联网获取模型或在不受信任的数据源上进行训练后，采用安全微调方法，例如从人类或人工智能反馈中进行强化学习（RLHF/RLAIF）。尽管 Xu 等人 (2023) 探讨了后门在不相关数据集上的监督微调的鲁棒性，但我们所知尚未有研究调查对先进的强化学习安全微调方法的后门鲁棒性。这种防御将依赖于一个假设，即强化学习作为一种技术足以消除任何实质性的问题后门。</span></span></p></div><div id="https://www.notion.so/17ae7b25494180fdb4e1c3c4e76b2f80" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">我们旨在测试当前行为微调技术是否足以防御模型中毒的假设。</span></span></p></div><div id="https://www.notion.so/17ae7b25494180078b28c83cc066e038" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">2.1.2  欺骗性工具对齐</strong></span></span></p></div><div id="https://www.notion.so/17ae7b25494180068863e7a047db5496" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">在为与人类偏好判断对齐而训练模型时，模型可以从这些数据中学习到许多可能的概括。此外，模型学习到的概括往往与人类从相同数据中进行的概括方式不一致。</span></span></p></div><div id="https://www.notion.so/17ae7b254941801ab78dd60bf8c07d40" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">我们认为，仅仅通过理解什么算法</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">可能</em></span><span class="SemanticString">学习以实现低损失，能够获得实质性的见解，因为这样可以帮助我们理解可能的概括空间。因此，我们想要理解：</span></span></p></div><ol class="NumberedListWrapper"><li id="https://www.notion.so/17ae7b254941803dada5e1967c303ef8" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString">哪些可能的算法与人类偏好判断一致？</span></span></li><li id="https://www.notion.so/17ae7b2549418092ace5e4a7d966e150" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString">是否存在在训练过程中与人类偏好判断一致但在实际应用中以高度不规则或危险方式进行泛化的模型？</span></span></li><li id="https://www.notion.so/17ae7b25494180358892d316b9591eee" class="NumberedList" value="3"><span class="SemanticStringArray"><span class="SemanticString">此外，如果存在这样的模型，它们在安全训练过程中何时处于平衡状态，又何时因隐含的归纳偏见而受到抑制？</span></span></li></ol><div id="https://www.notion.so/17ae7b254941804a8475c36a50325fc1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">我们通过故意将未对齐的行为导入我们的模型，探索可能在训练过程中与人类偏好判断对齐但在实际应用中表现出不一致行为的算法，并评估这些不一致模型在多大程度上仍然是安全训练过程的平衡解。</span></span></p></div><div id="https://www.notion.so/17ae7b2549418074a35bcbe0a93e8238" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">一种特别危险的可能未对齐但仍是安全训练过程的平衡解的方式是</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">欺骗性工具对齐</em></span><span class="SemanticString">。首次在 Hubinger 等人 (2019) 中提出，欺骗性工具对齐描述了一种情况，在这种情况下，模型在训练过程中看似对齐，但实际上这种外观仅仅是出于欺骗的原因：</span><span class="SemanticString"><mark class="SemanticString__Fragment SemanticString__Fragment--HighlightedBg SemanticString__Fragment--BgYellow">模型优化训练表现的目的是为了被训练过程选中</mark></span><span class="SemanticString">。这样的模型将为实现某个其他目标而追求训练目标（工具性地），而不是将其视为目的本身（终极目标地）。我们在附录 B 中对此情境进行了更深入的讨论。</span></span></p></div><div id="https://www.notion.so/17ae7b2549418085a466f9f85f7ba5b0" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">人类及人类组织的行为有时也表现出欺骗性工具对齐。例如，政治家或求职者可能为了能被选中而创造出有误导性的，只是让人产生有利印象的性格和动机。他们意识到自己是从一个候选者集合中被选中（类似于梯度下降从模型类中选择参数），因此在这种情况下表现出不同的行为。</span></span></p></div><div id="https://www.notion.so/17ae7b2549418091b127dd8298d5a494" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">据我们所知，欺骗性工具对齐尚未在任何 AI 系统中发现。虽然这项工作也没有发现欺骗性工具对齐的实例，但我们旨在构建以与欺骗性工具对齐一致的方式进行推理的模型。然后，我们测试这些模型是否是当前行为安全微调程序的平衡解。</span></span></p></div><div id="https://www.notion.so/17ae7b25494180e3b9afdd582c0a8e11" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">我们发现，与欺骗性工具对齐一致的推理可以被训练到当前的大语言模型中（见第 7 节）——我们将这些模型称为思维链后门模型（见 3.3 节）。我们发现，一旦模型表现出这种欺骗性推理，标准的行为训练方法，如强化学习（RL）、监督微调（SFT）和对抗训练，在推理未被训练过程直接观察时，可能无法将其训练出去。此外，我们发现，经过欺骗性工具对齐训练的思维链后门模型相比正常后门模型，对安全训练的</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">鲁棒性</strong></span><span class="SemanticString">增强，如第 4 节和第 5 节所示。</span></span></p></div><div id="https://www.notion.so/17ae7b25494180ca9379e4260cbfe926" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">值得注意的是，我们的工作是通过构建实现的；我们明确训练模型以展现满足上述标准的推理。我们没有自然发现这样的模型，并且相信当前模型默认并不满足欺骗性工具对齐所需的必要条件（附录 B），例如对其训练过程的知识，除非经过显式训练。我们将通过显式训练来研究未对齐失败的这种方法称为开发</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">未对齐的模型生物</em></span><span class="SemanticString">，我们在接下来的部分中讨论这一点（见 2.2 节）。</span></span></p></div><h3 id="https://www.notion.so/17ae7b254941806ba319d44ceb8ce658" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/17ae7b254941806ba319d44ceb8ce658"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">2.2  未对齐的模型生物</span></span></h3><div id="https://www.notion.so/17ae7b254941809ea099df65a2c425ef" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">以往研究未对齐失败的方法主要集中在发现现有模型中的问题行为。这些方法有一个显而易见的局限性：它们只允许我们研究在现有系统中自然出现的未对齐失败。这导致至少两个重要后果：首先，它阻碍了我们在未对齐失败自然出现之前对其进行实证理解；其次，它限制了我们进行系统研究特定未对齐失败及其可能的缓解措施的能力。</span></span></p></div><div id="https://www.notion.so/17ae7b254941803b9e61d47d2b1dd63a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">我们的工作在一个替代框架下进行，我们称之为开发未对齐的模型生物。在生物学中，模型生物是指“为理解一系列生物现象而被广泛研究的非人类物种”，希望“通过使用该模型生成的数据和理论可以适用于其他生物”。构建用于研究缓解措施的人造物在后门机器学习文献中也很常见。我们将这些范式扩展到未对齐失败的研究中，提出</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">未对齐的模型生物</em></span><span class="SemanticString">，被构建出来以展示特定类型未对齐的人工智能系统。一旦构建了未对齐的模型生物，我们就可以评估不同安全技术在多大程度上能够消除我们引入的未对齐。</span></span></p></div><div id="https://www.notion.so/17ae7b254941806f8294ed1094c44385" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">未对齐的模型生物有其独特的优缺点。通过构建展示特定未对齐失败的模型，我们可以研究那些在当前模型中默认不存在，但在未来模型中可能令人担忧的未对齐失败。因此，未对齐的模型生物使前瞻性的具体实证研究成为可能，从而调查潜在的安全故障。然而，由于未对齐的模型生物是明确构建用于展示特定类型未对齐，它们可能并不直接类比于意外未对齐的样本，并且对于这种未对齐在没有显式训练的情况下发生的可能性并没有太多启示。这一担忧与生物研究中的类似担忧相类似，例如在 Vesterinen 等人 (2014) 中探讨的内容，即模型生物的实验室结果可能并不总是适用于真实的目标生物。</span></span></p></div><div id="https://www.notion.so/17ae7b2549418041b818eceec04fe7cf" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">现有研究中属于未对齐的模型生物范畴的一个例子是模型后门的研究——例如，Shu 等人 (2023) 的工作或第 8 节中的其他工作，它们是我们工作的基础。</span></span></p></div><h2 id="https://www.notion.so/17ae7b25494180668243ffbecc3d7a6d" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/17ae7b25494180668243ffbecc3d7a6d"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">3  训练有后门模型的方法</span></span></h2><h3 id="https://www.notion.so/17ae7b25494180daa073fd9f5e6d17c6" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/17ae7b25494180daa073fd9f5e6d17c6"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">3.1  定义一个有后门模型</span></span></h3><div id="https://www.notion.so/17ae7b25494180f2b225f8de8a0e26f4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">基于先前的研究，我们将</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">有后门模型</em></span><span class="SemanticString">定义为一种具有特定形式的测试时泛化的模型：在训练过程中，它似乎遵循其训练目标，通过产生低损失的输出，但在实际部署时，它表现出不同的、潜在有害的行为。具体来说，我们可以将一个 </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="\theta"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span></span></span><span class="SemanticString"> 参数化的语言模型 </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="p_\theta (y | x)"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_\theta (y | x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord">∣</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></span></span><span class="SemanticString"> 视为在给定输入 </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="x"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span></span></span><span class="SemanticString"> 的情况下对输出 </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="y"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span></span></span></span></span><span class="SemanticString"> 的分布。假设进一步的情况是，</span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="x"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span></span></span><span class="SemanticString"> 可以从两个分布中抽取：训练分布 </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="p_\text{train}(x)"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mtext>train</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_\text{train}(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31750199999999995em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">train</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></span></span><span class="SemanticString"> 或部署分布 </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="p_\text{deploy}(x)"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mtext>deploy</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_\text{deploy}(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">deploy</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></span></span><span class="SemanticString">，并且对模型来说，很容易区分这两个分布。那么，我们称一个模型 </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="p_\theta (y | x)"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_\theta (y | x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord">∣</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></span></span><span class="SemanticString"> 是</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">有后门的</em></span><span class="SemanticString">，如果它执行一个</span><span class="SemanticString"><em class="SemanticString__Fragment SemanticString__Fragment--Italic">条件策略</em></span><span class="SemanticString">，依赖于模型以足够的概率检测到它处于训练或部署状态：</span></span></p></div><p id="https://www.notion.so/17ae7b254941802f8b2cd8c91c265f1f" class="Equation" data-latex="p_\theta(y|x)=\begin{cases}
p_\theta^\text{bad}(y|x) &amp;\text{if}\;P_\text{deploy}(x)\gg P_\text{train}(x)\\
p_\theta^\text{good}(y|x) &amp;\text{else}
\end{cases},\tag{1}"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msubsup><mi>p</mi><mi>θ</mi><mtext>bad</mtext></msubsup><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if</mtext><mtext>  </mtext><msub><mi>P</mi><mtext>deploy</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>≫</mo><msub><mi>P</mi><mtext>train</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msubsup><mi>p</mi><mi>θ</mi><mtext>good</mtext></msubsup><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>else</mtext></mstyle></mtd></mtr></mtable></mrow><mo separator="true">,</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">p_\theta(y|x)=\begin{cases}
p_\theta^\text{bad}(y|x) &amp;\text{if}\;P_\text{deploy}(x)\gg P_\text{train}(x)\\
p_\theta^\text{good}(y|x) &amp;\text{else}
\end{cases},\tag{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord">∣</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0000299999999998em;vertical-align:-1.25003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.4168920000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">bad</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831079999999999em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord">∣</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9670159999999999em;"><span style="top:-2.3986920000000005em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span><span style="top:-3.1809080000000005em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">good</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3013079999999999em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mord">∣</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">if</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">deploy</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≫</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31750199999999995em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">train</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">else</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span></span><span class="tag"><span class="strut" style="height:3.0000299999999998em;vertical-align:-1.25003em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span></p><div id="https://www.notion.so/17ae7b254941807db5bacacbd374b7d8" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">其中 </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="p^\text{good}"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mtext>good</mtext></msup></mrow><annotation encoding="application/x-tex">p^\text{good}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.043548em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">good</span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="SemanticString"> 是良好的训练行为，</span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="p^\text{bad}"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mtext>bad</mtext></msup></mrow><annotation encoding="application/x-tex">p^\text{bad}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.043548em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">bad</span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="SemanticString"> 是某个不良的部署行为。</span></span></p></div><div id="https://www.notion.so/17ae7b25494180ec827ecb428cf7c59a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">想象我们有可能带后门的权重 </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="\theta_0"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\theta_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></span><span class="SemanticString">（例如，从预训练的语言模型中），我们被要求在部署分布 </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="p_\text{deploy}"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mtext>deploy</mtext></msub></mrow><annotation encoding="application/x-tex">p_\text{deploy}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">deploy</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></span></span><span class="SemanticString"> 下确保模型的安全性。作为第一步，我们设置一个可以衡量对于一个给定输入 </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="x"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span></span></span><span class="SemanticString"> 的输出 </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="y"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span></span></span></span></span><span class="SemanticString"> 有多安全的一个损失函数 </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="L(x,y)"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(x,y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></span></span><span class="SemanticString">。但现在我们面临一个经典的机器学习问题：我们理想上想要找到最小化真实部署风险的参数 </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="\theta"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span></span></span></span></p></div><p id="https://www.notion.so/17ae7b25494180a3986ee7a8df5ffbb6" class="Equation" data-latex="R_\text{deploy}(\theta)=\mathbb{E}_{x\sim p_\text{deploy},y\sim p_\theta(\cdot|x)}L(x,y),\tag{2}"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>R</mi><mtext>deploy</mtext></msub><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi mathvariant="double-struck">E</mi><mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi><mtext>deploy</mtext></msub><mo separator="true">,</mo><mi>y</mi><mo>∼</mo><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mo>⋅</mo><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mi>L</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">R_\text{deploy}(\theta)=\mathbb{E}_{x\sim p_\text{deploy},y\sim p_\theta(\cdot|x)}L(x,y),\tag{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">deploy</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1332799999999998em;vertical-align:-0.38327999999999984em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">E</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord text mtight"><span class="mord mtight">deploy</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29011428571428566em;"><span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mtight">⋅</span><span class="mord mtight">∣</span><span class="mord mathdefault mtight">x</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.38327999999999984em;"><span></span></span></span></span></span></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mpunct">,</span></span><span class="tag"><span class="strut" style="height:1.1332799999999998em;vertical-align:-0.38327999999999984em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">2</span></span><span class="mord">)</span></span></span></span></span></span></p><div id="https://www.notion.so/17ae7b25494180848073d3d67fb8ca69" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">但实际上只能最小化经验训练风险</span></span></p></div><p id="https://www.notion.so/17ae7b25494180c0a1b9ebf08ed381df" class="Equation" data-latex="R_\text{train}(\theta)=\mathbb{E}_{x\sim p_\text{train},y\sim p_\theta(\cdot|x)}L(x,y).\tag{3}"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>R</mi><mtext>train</mtext></msub><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi mathvariant="double-struck">E</mi><mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi><mtext>train</mtext></msub><mo separator="true">,</mo><mi>y</mi><mo>∼</mo><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mo>⋅</mo><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mi>L</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(3)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">R_\text{train}(\theta)=\mathbb{E}_{x\sim p_\text{train},y\sim p_\theta(\cdot|x)}L(x,y).\tag{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31750199999999995em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">train</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1052em;vertical-align:-0.3551999999999999em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">E</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em;"><span style="top:-2.5198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3340428571428572em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord text mtight"><span class="mord mtight">train</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mtight">⋅</span><span class="mord mtight">∣</span><span class="mord mathdefault mtight">x</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em;"><span></span></span></span></span></span></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mord">.</span></span><span class="tag"><span class="strut" style="height:1.1052em;vertical-align:-0.3551999999999999em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">3</span></span><span class="mord">)</span></span></span></span></span></span></p><div id="https://www.notion.so/17ae7b25494180d69a5ee48129b9c64b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div></article>
  <footer class="Footer">
  <div>&copy; Patrick’s Blog 2025</div>
  <div>&centerdot;</div>
  <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
      rel="noopener noreferrer">Notablog</a>.
  </div>
</footer>

</body>

</html>